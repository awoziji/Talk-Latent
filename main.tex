\documentclass[aspectratio=169]{beamer}
% \mode<presentation>
\setbeamertemplate{navigation symbols}{}
\let\tempone\itemize
\let\temptwo\enditemize
\renewenvironment{itemize}{\tempone\addtolength{\itemsep}{0.5\baselineskip}}{\temptwo}

%%%%%%%%%%%%%%%%%%%%%%
\usepackage{beamerthemeshadow}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pgffor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{tabularx}
\usepackage{tikz,etoolbox}
\usepackage{tikz,amsmath,siunitx}
\usetikzlibrary{arrows,snakes,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\usepackage{subcaption}
\usepackage{pgf}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage[absolute,overlay]{textpos}
\usetikzlibrary{shapes,arrows,positioning,automata,positioning,spy,matrix,scopes,chains}
\newcommand{\digs}[2]{\hphantom{999}\llap{#1}\,+\,\hphantom{999}\llap{#2}}
\setbeamersize{text margin left=6mm}
\setbeamersize{text margin right=6mm}
\renewcommand{\insertnavigation}[1]{}
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}
\usefonttheme{professionalfonts}
\setbeamercovered{transparent}
\mode<presentation>
\linespread{1.25}
\newcommand{\thetitle}[1]{{\begin{center}\textbf{{#1}}\end{center}}}
\input{math.tex}
\usepackage{color}
\usepackage{blindtext}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[all,dvips]{xy}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{natbib}
\usepackage[labelformat=empty]{caption}
\newcommand{\air}{\vspace{0.25cm}}
\newcommand{\mair}{\vspace{-0.25cm}}

\setbeamertemplate{navigation symbols}{
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
%remove navigation symbols
\renewcommand{\rmdefault}{crm}
\definecolor{vermillion}{RGB}{213,94,0}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{skyblue}{RGB}{86,180,233}
\definecolor{bluegreen}{RGB}{90,143,41}
% \definecolor{bluegreen}{RGB}{0,158,115}
\definecolor{myyellow}{RGB}{240,228,66} % i dunno if this is the same as standard yellow
\definecolor{myblue}{RGB}{0,114,178}
\definecolor{vermillion}{RGB}{213,94,0}
\definecolor{redpurple}{RGB}{204,121,167}
\definecolor{lightgrey}{RGB}{234,234,234}
\usepackage{tikz}
\usetikzlibrary{fit,positioning}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathreplacing}
% \setbeamerfont{alerted text}{series=\bfseries}
% \setbeamerfont{structure}{series=\bfseries}
% Needed for diakgrams.
\def\im#1#2{
  \node(#1) [scale=#2]{\pgfbox[center,top]{\pgfuseimage{#1}}
};}
% \input{pictures_header}
% make smaller citations
\let\realcitep\citep
\renewcommand*{\citep}[1]{{\scriptsize \realcitep{#1}}}
\let\realcitet\citet
\renewcommand*{\citet}[1]{{\scriptsize \realcitet{#1}}}
\setcitestyle{square,semicolon,aysep={}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Controlling Text Generation]{Controlling Text Generation}
\author[]{Alexander Rush \\(with Yoon Kim, Sam Wiseman, Sebastian Gehrmann,\\ Yuntian Deng, Justin Chiu, and Demi Guo)}
\institute[Harvard SEAS]{
  \begin{center}
    \includegraphics[width=5cm]{harvardnlp}
  \end{center}
  \vspace{0.5cm}
  {\Large New York 2018}\\

  { \url{nlp.seas.harvard.edu} / @harvardnlp}
}
\date{}
%\usetheme{Madrid}
%\usetheme[hideothersubsections]{Hannover}
\definecolor{darkgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{darkpurple}{rgb}{0.55, 0.0, 0.55}

\AtBeginSection[]
{
  \begin{frame}
  \tableofcontents[currentsection,hideothersubsections]
  \end{frame}
}

\AtBeginSubsection[]
{
  \begin{frame}
  \tableofcontents[currentsection,
        currentsubsection,
        subsectionstyle=show/shaded/hide]
  \end{frame}
}


\def\argmax{\operatornamewithlimits{arg\,max}}
\def\kargmax{\operatornamewithlimits{K-arg\,max}}
\setbeamercovered{transparent}


\begin{document}

\maketitle

\section{Preface: Seq2Seq+}


\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+}  \air
  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn1}
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn2}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn3}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn4}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn5}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn6}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn7}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn8}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn9}
\end{frame}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn10}
\end{frame}

\begin{frame}
  \centerline{\textbf{Seq2Seq+}}

  \textcolor{blue}{Encoder} :
  \[{\mathbf{h}^{x}_m \gets \RNN(\mathbf{h}^{x}_{m-1}, x_m)} \]


  \textcolor{orange}{Attention}
  \[\alpha \gets  \softmax(   [\mathbf{h}^{x}_1 ; \ldots; \mathbf{h}^{x}_M]^\top \mathbf{h}_{n} )\]
  \[{\mathbf{c}_n} \gets \sum_{m =1}^M \alpha_m \mathbf{h}_m^{x}  \]

  \textcolor{red}{Decoder}:
  \[{\mathbf{h}_n \gets \RNN(\mathbf{h}_{n-1}, w_n)} \]

  Prediction
  \[p(w_{n+1} | w_{1:n}, x_{1:M}) = \softmax( \mathbf{W} [\mathbf{h}_n, \mathbf{c}_n]) \]

\end{frame}


\begin{frame}
  \begin{center}
    \textbf{OpenNMT\\ {\small (Klein et al, 2017)}}

  \includegraphics[width=10cm]{opennmt}
  \end{center}

\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Seq2Seq-Vis \\ {\small (Strobelt et al, 2018)} }
  \end{center}

  \includegraphics[width=\textwidth]{s2stease}
\end{frame}


\section{Introduction: Data-Driven Text Generation}

\begin{frame}
  \begin{center}
    \textbf{The Modern Text Generation Challenge}
  \end{center}
  \begin{center}
\begin{tikzpicture}
\node{\includegraphics[width=5cm]{galaxy}};
\end{tikzpicture}
  \end{center}
\end{frame}

% \begin{frame}
%   \begin{center}
%     \structure{The Modern Text Generation Challenge: \\ Make Some Conversation (Chatbots)}
%   \end{center}
%   \begin{center}
% \begin{tikzpicture}
% \node{\includegraphics[width=5cm]{galaxy}};
% \path[draw] (0, 0) --  (2,0) ;
% \node [xshift=3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize Hi there Joe, have a good day today!  \par};
% \end{tikzpicture}
%   \end{center}
% \end{frame}


\begin{frame}
  \begin{center}
    \textbf{Text Generation: \\ Talk about Text (Translation / Summarization) }
  \end{center}
  \begin{center}
\begin{tikzpicture}
\node{\includegraphics[width=5cm]{galaxy}};
\path<2>[draw] (0, 0) --  (2,0) ;

\node[inner sep=1pt, rounded corners, text width=15em, xshift=-4cm,]{\small

\vspace{0.5cm}
\tiny
mexico city , mexico -lrb- cnn -rrb- -- heavy rains and flooding have forced hundreds of thousands of people from homes in southern mexico 's state of tabasco over the past four days , with nearly as many trapped by the rising waters , state officials said thursday . officials say about 300,000 people are still trapped by the worst flooding in the region for 50 years . the grijalva river pushed over its banks through the state capital of villahermosa on thursday , forcing government workers to evacuate and leaving up to 80 percent of the city flooded , gov. andres granier 's office told cnn . about 700,000 people have seen their homes flooded , with about 300,000 of those still trapped there , granier 's office reported . one death had been blamed on the floods , which followed weeks of heavy rain in the largely swampy state . tabasco borders guatemala to the south and the gulf of mexico to the north . \ldots

};
\visible<2>{
\node [xshift=3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize tabasco and chiapas states hardest hit. authorities say 700,000 affected \ldots   \par};}
\end{tikzpicture}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \textbf{Text Generation: \\ Talk about Structured Data (Generation) }
  \end{center}
  \begin{center}
\begin{tikzpicture}
\node{\includegraphics[width=5cm]{galaxy}};
\path<2>[draw] (0, 0) --  (2,0) ;
\node[inner sep=1pt, rounded corners, text width=15em, xshift=-4cm,]{\small
  \begin{center}
\vspace{0.5cm}
\footnotesize
\begin{tabular}{lcccccc}
\toprule
{} & W & L & PTS &  \ldots \\
TEAM &           &             &          &                      \\
\midrule
Heat   &        11 &          12 &      103 &     \ldots      \\
Hawks  &         7 &          15 &       95 &     \ldots      \\
\bottomrule

\vspace*{0.3cm}
\end{tabular}

% \begin{tabular}{lccccccccc}
% \toprule
% {} &  AS &    RB &   PT &  FG &  FGA & CITY  $\ldots$ \\
% PLAYER      &      &      &      &       &      &      &           \\
% \midrule
% Tyler Johnson    &    5 &    2 &  27 &    8 &   16 &     Miami \\
% Dwight Howard    &    11 &    17 &  23 &    9 &   11 &   Atlanta \\
% Paul Millsap     &    2 &    9 &  21 &    8 &   12 &   Atlanta \\
% Goran Dragic     &    4 &    2 &  21 &    8 &   17 &     Miami \\
% Wayne Ellington  &    2 &    3 &  19 &    7 &   15 &     Miami \\
% Dennis Schroder  &    7 &    4 &  17 &    8 &   15 &   Atlanta \\
% Rodney McGruder  &    5 &    5 &  11 &    3 &    8 &     Miami \\
% \ldots \\
% \bottomrule
% \end{tabular}

  \end{center}




};
 % \node [xshift=-3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize The Atlanta Hawks defeated the Miami Heat, 103 - 95, at Philips Arena on Wednesday. Atlanta  ... \par};
\visible<2>{\node[xshift=3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize The Atlanta Hawks defeated the Miami Heat, 103 - 95, at Philips Arena on Wednesday. Atlanta  ... \par};}
\end{tikzpicture}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \textbf{ Text Generation: \\ Talk about the Environment (Multimodal) }
  \end{center}
  \begin{center}
\begin{tikzpicture}
\node{\includegraphics[width=5cm]{galaxy}};
\path<2>[draw] (0, 0) --  (2,0) ;
 \node [xshift=-4cm, rectangle, thick,fill=blue!0,text width=5cm, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \includegraphics[width=5cm]{dowjones}};
\visible<2>{
\node [xshift=3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize Dow and S\&P 500 close out week at all-time highs ... \par};}
\end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Are these problems just Seq2Seq?}

    \begin{itemize}
    \item Certainly the key aspect of their success.
      \air

    \item However there has slowly tentative steps towards problem
      specific modeling.
    \end{itemize}
  \end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \textbf{Case Study: Seq2Seq+Copy}

    \small{(Gulcehre et al, 2016) ...}
  \end{center}
  \textcolor{blue}{Encoder} :
  \[{\mathbf{h}^{x}_m \gets \RNN(\mathbf{h}^{x}_{m-1}, x_m)} \]


  \textcolor{orange}{Attention}
  \[\alpha \gets  \softmax(   [\mathbf{h}^{x}_1 ; \ldots; \mathbf{h}^{x}_M]^\top \mathbf{h}_{n} )\ \ \ {\mathbf{c}_n} \gets \sum_{m =1}^M \alpha_m \mathbf{h}_m^{x}  \]

  \textcolor{red}{Decoder}:
  \[{\mathbf{h}_n \gets \RNN(\mathbf{h}_{n-1}, w_n)} \]

  Prediction
  \begin{eqnarray*}
  p(w_{n+1} | w_{1:n}, x_{1:M}) = & p(z = 1 |  w_{1:n}, x_{1:M}) \times \softmax( \mathbf{W} [\mathbf{h}_n, \mathbf{c}_n])   \\+ &\displaystyle p(z=0 | w_{1:n}, x_{1:M}) \times \sum_{m: x_m = w_{n+1}} \alpha_m
  \end{eqnarray*}
\end{frame}

\begin{frame}
\begin{center}
    \textbf{Copy is Critical For Summarization }
    \\
    \small{(See et al. 2017)}
      \includegraphics[width=15cm]{seeblog}
\pause

      \textbf{Bottom-Up Summarization}

 (Gehrmann et al, 2018)


 \begin{itemize}
 \item Shows that controlling copying alone produces state-of-the-art
   results.
 \end{itemize}
\end{center}
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Challenges of Neural Generation  \\ {\small (Wiseman et al, 2017) } }
  \end{center}
  \begin{figure}
\centering
\scalebox{0.6}{
\begin{tikzpicture}
\node[draw, inner sep=10pt, rounded corners, text width=30em, xshift=-7.5cm,]{\small
  \begin{center}


\begin{tabular}{lcccccc}
\toprule
{} & WIN & LOSS & PTS & FG\_PCT & RB & AS \ldots \\
TEAM &           &             &          &             &          &          \\
\midrule
Heat      &        11 &          12 &      103 &          49 &       47 &       27 \\
Hawks     &         7 &          15 &       95 &          43 &       33 &       20 \\
\bottomrule
\end{tabular}
\vspace{0.5cm}

\begin{tabular}{lccccccccc}
\toprule
{} &  AS &    RB &   PT &  FG &  FGA & CITY  $\ldots$ \\
PLAYER      &      &      &      &       &      &      &           \\
\midrule
Tyler Johnson    &    5 &    2 &  27 &    8 &   16 &     Miami \\
Dwight Howard    &    11 &    17 &  23 &    9 &   11 &   Atlanta \\
Paul Millsap     &    2 &    9 &  21 &    8 &   12 &   Atlanta \\
Goran Dragic     &    4 &    2 &  21 &    8 &   17 &     Miami \\
Wayne Ellington  &    2 &    3 &  19 &    7 &   15 &     Miami \\
Dennis Schroder  &    7 &    4 &  17 &    8 &   15 &   Atlanta \\
Rodney McGruder  &    5 &    5 &  11 &    3 &    8 &     Miami \\
\ldots \\
\bottomrule
\end{tabular}

  \end{center}

};


\node [yshift=-4cm, rectangle, draw,thick,fill=blue!0,text width=30em, rounded corners, inner sep =10pt, minimum height=1em]{\baselineskip=100pt \large  The Atlanta Hawks defeated the Miami Heat, 103 - 95, at Philips Arena on Wednesday. Atlanta was in desperate need of a win and they were able to take care of a shorthanded Miami team here. Defense was key for the Hawks, as they held the Heat to 42 percent shooting and forced them to commit 16 turnovers. Atlanta also dominated in the paint, winning the rebounding battle, 47 - 34, and outscoring them in the paint 58 - 26. The Hawks shot 49 percent from the field and assisted on 27 of their 43 made baskets. This was a near wire-to-wire win for the Hawks, as Miami held just one lead in the first five minutes. Miami ( 7 - 15 ) are as beat-up as anyone right now and it's taking a toll on the heavily used starters. Hassan Whiteside really struggled in this game, as he amassed eight points, 12 rebounds and one blocks on 4 - of - 12 shooting ... \par};
\end{tikzpicture}
}
\caption{\small Document generation example. Left, a source consisting of structured data, in this case statistics from a basketball game. Right, a target document utilizing a select subset of records from the data,
  expressed in an often complex and expressive manner. This example is an excerpt from the case-study data set, which include
  628 records in total and a longer target document. }
\label{fig:samplesummary}
\end{figure}


\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Challenges of Neural Generation  \\ {\small (Wiseman et al, 2017) } }
  \end{center}

  \begin{figure}
\centering
\scalebox{0.9}{\begin{tikzpicture}
    \node[text width=12cm, draw, rounded corners, inner sep=10pt]{\small
  The \textcolor<2>{blue}{Utah Jazz} ( \textcolor<2>{blue}{38} - \textcolor<2>{red}{26} ) \textcolor<2>{red}{defeated} the \textcolor<2>{blue}{Houston Rockets} ( \textcolor<2>{red}{38} - \textcolor<2>{blue}{26} ) \textcolor<2>{blue}{117 - 91} on Wednesday at \textcolor<2>{blue}{Energy Solutions Arena in Salt Lake City} . The \textcolor<2>{blue}{Jazz} got out to a quick start in this one , out - scoring the \textcolor<2>{blue}{Rockets} \textcolor<2>{blue}{31} - \textcolor<2>{red}{15} in the first quarter alone . Along with the quick start , the \textcolor<2>{blue}{Rockets} were the superior shooters in this game , going \textcolor<2>{blue}{54 percent from the field} and \textcolor<2>{blue}{43 percent from the three - point line} , while the \textcolor<2>{blue}{Jazz} went \textcolor<2>{blue}{38} percent from the floor and a meager \textcolor<2>{blue}{19} percent from deep . The \textcolor<2>{blue}{Rockets} were able to out - rebound the \textcolor<2>{red}{Rockets} \textcolor<2>{blue}{49} - \textcolor<2>{red}{49} , giving them just enough of an advantage to secure the victory in front of their \textcolor<2>{red}{home crowd} . The \textcolor<2>{blue}{Jazz} were led by the duo of \textcolor<2>{blue}{Derrick Favors} and \textcolor<2>{red}{James Harden} . Favors went \textcolor<2>{blue}{2 - for - 6} from the field and \textcolor<2>{blue}{0 - for - 1} from the three - point line to score a \textcolor<2>{red}{game - high} of \textcolor<2>{red}{15 points} , while also adding \textcolor<2>{blue}{four rebounds} and \textcolor<2>{red}{four assists} ....};
  \end{tikzpicture}}
\end{figure}
\begin{center}
\end{center}
\end{frame}

\begin{frame}
  \centerline{\textbf{Quantitative Evaluation / RotoWire}}
  \begin{table}
    \small
    \centering
    \begin{tabular}{lcccccc}
      \toprule
       & \multicolumn{5}{c}{} \\
       & \multicolumn{2}{c}{Relations}  & \multicolumn{2}{c}{Content} & Ordering \\
      Model & P\% & \# & P\% & R\% & DLD\%  & BLEU\\
      \midrule
      % &Gold                     & 91.77 & 12.84 & 100   & 100  & 100   \\
      Template                 & \textbf{99.35} & 49.7  & \textbf{45.17} & 24.85 & \textbf{12.2} & 6.87    \\
      \midrule
      Seq2Seq+Copy        &   71.07 & 12.61 & 21.90 & 27.27 & 8.70 & \textbf{14.46}\\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}


\begin{frame}
  \begin{center}
    \textbf{ What is Missing for  Generation? }
  \end{center}

    \textit{Building Natural Language Generation Systems} (Reiter and Dale, 1999)

\vspace*{-1cm}
 \begin{center}
    \includegraphics[width =12cm]{datadoc}
  \end{center}
  \vspace*{-0.5cm}


  \begin{itemize}
  \item Argument 1: Need bigger models. At greater than X parameters these
    properties will start showing up. (May likely be true)
    \air
  \item Argument 2: Just like \textit{copy} we need richer models of
    the underlying phenomenon.

  \end{itemize}


\end{frame}



\section{Latent-Variable Generation}

\begin{frame}
\begin{center}
    \textbf{Research Direction: \\
      Deep Latent-Variable Models for NLP }
  \end{center}
  Goal: Expose specific choices as explicit \textit{discrete} latent variables.


\begin{align*}
p(y, z \param \theta).
\end{align*}

\pause
\begin{itemize}
    \item $y$ is our observed data
    \item $z$ is a collection of problem-specific latent variables
    \item $\theta$ are the deterministic, neural network parameters.
\end{itemize}

% \begin{itemize}
%     \item Data consists of $N$ i.i.d samples,
% \end{itemize}

%                 \[ p(x^{(1:N)}, z^{(1:N)} \param \theta ) = \prod_{n=1}^N p(x^{(n)} \given z^{(n)}; \theta) p(z^{(n)};\theta). \]

\end{frame}

\begin{frame}
\thetitle{Example Model: Mixture of RNNs}

Generative process:
\begin{enumerate}
\item Draw cluster $z \in \{1, \ldots, K\}$ from a Categorical.
\item Draw words $y_{1:T}$ from RNNLM with parameters $\pi_z$.
\end{enumerate}
\[p(y, z \param \theta)
       = \mu_{z} \times   \RNNLM(y_{1:T} \param \pi_z) \]
% \[p(x, z \param \theta)
%       = \mu_{z} \times  \prod_{t=1}^T \softmax(\RNN(\boldh_{t-1}, x_t\param \pi_z))\]
\begin{center}

\begin{tikzpicture}
  %\tikz{
% nodes
\node (dots) {$\ldots$};%
 \node[obs, left=1cm of dots] (x1) {$y_1^{(n)}$};%
 \node[obs, right=1cm of dots] (xT) {$y_T^{(n)}$};%
 \node[latent, above=of dots] (z) {$z^{(n)}$}; %
 \node[const, above=(0.5cm) of z] (mu) {$\mu$};
 \node[const, below left=0.3cm and 0.8cm of x1] (pi) {$\pi$};

% plate
 \plate {plate1} {(dots)(x1)(xT)(z)} {$N$}; %
% edges
 \edge {z} {dots};
 \edge {z} {x1};
 \edge {z} {xT};
 \edge {mu} {z};
 \edge {pi.east} {x1,xT.south};
 \edge {x1} {dots};
 %\edge[bend left] {x1.south} {xT.south};
  \edge {dots} {xT};

 \draw[->]
 (x1) edge[bend right=10] node [right] {} (xT);
 %}
 \end{tikzpicture}
 %}
\end{center}
%\begin{align*}
%\boldh_{z,t} &= \tanh(\mathbf{W}_z \bolde_t +\mathbf{U}_z\boldh_{z,t-1}  + \boldb_{z}) \nonumber \\
%p(x_{t} \given x_{<t} , z) &= \softmax(\mathbf{V} \boldh_{z,t-1} + \boldc)_{x_{t}} \nonumber \\
%p(x_1, \ldots, x_T \given z) &= \prod_{t=1}^{T} p(x_{t} \given x_{<t} , z)
%\end{align*}


\end{frame}


% begin{frame}
% \begin{center}
%     \textbf{ Latent-Variable Model Basics }
%   \end{center}


% \begin{align*}
% p(x, z \param \theta).
% \end{align*}

% \pause
% \begin{itemize}
%     \item $x$ is our observed data
%     \item $z$ is a collection of latent variables
%     \item $\theta$ are the deterministic parameters of the model, such as the neural network parameters
% \end{itemize}

% \pause

% \begin{itemize}
%     \item Data consists of $N$ i.i.d samples,
% \end{itemize}


%                 \[ p(x^{(1:N)}, z^{(1:N)} \param \theta ) = \prod_{n=1}^N p(x^{(n)} \given z^{(n)}; \theta) p(z^{(n)};\theta). \]
% \end{frame}

\begin{frame}\thetitle{Main Requirement: Posterior Inference}
    For models $p(y, z \param \theta)$, we'll be interested in the \textit{posterior} over latent variables $z$:

    \begin{align*}
        p(z \given y \param \theta) = \frac{\displaystyle p(y, z \param \theta)}{\displaystyle p(y \param \theta)} = \frac{\displaystyle p(y\given z \param  \theta) p(z \param  \theta)}{\displaystyle \sum_{z'} p(y \given z'\param  \theta) p(z'\param  \theta) }.
    \end{align*}

    \air

    \pause
    Why?
    \begin{itemize}
      \item Required for training
      \item Latent $z$ gives separation of data.

% \item Intuition: if I know likely $z^{(n)}$ for $x^{(n)}$, I can learn by maximizing $p(x^{(n)} \given z^{(n)} \param \theta)$.
        % \begin{itemize}
        %     \item Intuition: if I know likely $z^{(n)}$ for $x^{(n)}$, I can learn by maximizing $p(x^{(n)} \given z^{(n)} \param \theta)$.
        % \end{itemize}
    \end{itemize}

    How?

    \begin{itemize}
    \item Sum out over all discrete choices (e.g. run $K$ RNNs).
    \item Variational inference based methods.
    \end{itemize}

\end{frame}


\begin{frame}
  \begin{center}
    \textbf{ In Applications: Copy-Attention \\
      \small{(Gu et al, 2016) (Gulcehre et al, 2016)}}
  \end{center}

Let $z$ be a binary latent variable.
\air
\begin{itemize}
\item If $z = 1$, let the model generate a new word.
\item If $z = 0$, let the model copy a word from the source.
\end{itemize}

Inference:
\begin{center}


\includegraphics[width=15cm]{seeblog}

\centerline{\small (See et al, 2017)}
\end{center}
\end{frame}


\begin{frame}
  \begin{center}
    \textbf{ Latent Variable Models for Generation}
  \end{center}

  \begin{itemize}
  \item Can we develop other discrete latent-variable models for generation?
    \air
  \item Perhaps each important aspect of generation can be built-in directly.
    \air
  \item Goals:
    \begin{itemize}
    \item Model Control
    \item Model Debugging
    \item Model Uncertainty
    \end{itemize}
  \end{itemize}
\end{frame}





\begin{frame}
  \begin{center}
    \textbf{Approach 1: Learning Neural Templates}
  \end{center}

  \begin{center}
    \includegraphics[width=0.7\textwidth]{DecoderVis}
  \end{center}
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Approach 2: Latent Alignment and Variational Attention}
  \end{center}

  \begin{center}
    \includegraphics[width=0.7\textwidth]{AttentionVIS}
  \end{center}
\end{frame}

\section{Work 1: Learning Neural Templates}


\begin{frame}
  \begin{center}
    \textbf{Text Generation: \\ Talk about Structured Data (Generation) }
  \end{center}
  \begin{center}
\begin{tikzpicture}
\node{\includegraphics[width=5cm]{galaxy}};
\path<2>[draw] (0, 0) --  (2,0) ;
\node[inner sep=1pt, rounded corners, text width=15em, xshift=-4cm,]{\small
  \begin{center}
\vspace{0.5cm}
\footnotesize
\begin{tabular}{lcccccc}
\toprule
{} & W & L & PTS &  \ldots \\
TEAM &           &             &          &                      \\
\midrule
Heat   &        11 &          12 &      103 &     \ldots      \\
Hawks  &         7 &          15 &       95 &     \ldots      \\
\bottomrule

\vspace*{0.3cm}
\end{tabular}

% \begin{tabular}{lccccccccc}
% \toprule
% {} &  AS &    RB &   PT &  FG &  FGA & CITY  $\ldots$ \\
% PLAYER      &      &      &      &       &      &      &           \\
% \midrule
% Tyler Johnson    &    5 &    2 &  27 &    8 &   16 &     Miami \\
% Dwight Howard    &    11 &    17 &  23 &    9 &   11 &   Atlanta \\
% Paul Millsap     &    2 &    9 &  21 &    8 &   12 &   Atlanta \\
% Goran Dragic     &    4 &    2 &  21 &    8 &   17 &     Miami \\
% Wayne Ellington  &    2 &    3 &  19 &    7 &   15 &     Miami \\
% Dennis Schroder  &    7 &    4 &  17 &    8 &   15 &   Atlanta \\
% Rodney McGruder  &    5 &    5 &  11 &    3 &    8 &     Miami \\
% \ldots \\
% \bottomrule
% \end{tabular}

  \end{center}




};
 % \node [xshift=-3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize The Atlanta Hawks defeated the Miami Heat, 103 - 95, at Philips Arena on Wednesday. Atlanta  ... \par};
\visible<2>{\node[xshift=3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize The Atlanta Hawks defeated the Miami Heat, 103 - 95, at Philips Arena on Wednesday. Atlanta  ... \par};}
\end{tikzpicture}
  \end{center}
\end{frame}


{
\setbeamercolor{background canvas}{bg=}
\includepdf[pages=2-35]{template}
}

\section{Work 2: Learning Alignments}


\begin{frame}
  \begin{center}
    \textbf{Text Generation: \\ Talk about Text (Translation / Summarization) }
  \end{center}
  \begin{center}
\begin{tikzpicture}
\node{\includegraphics[width=5cm]{galaxy}};
\path<2>[draw] (0, 0) --  (2,0) ;

\node[inner sep=1pt, rounded corners, text width=15em, xshift=-4cm,]{\small

\vspace{0.5cm}
\tiny
mexico city , mexico -lrb- cnn -rrb- -- heavy rains and flooding have forced hundreds of thousands of people from homes in southern mexico 's state of tabasco over the past four days , with nearly as many trapped by the rising waters , state officials said thursday . officials say about 300,000 people are still trapped by the worst flooding in the region for 50 years . the grijalva river pushed over its banks through the state capital of villahermosa on thursday , forcing government workers to evacuate and leaving up to 80 percent of the city flooded , gov. andres granier 's office told cnn . about 700,000 people have seen their homes flooded , with about 300,000 of those still trapped there , granier 's office reported . one death had been blamed on the floods , which followed weeks of heavy rain in the largely swampy state . tabasco borders guatemala to the south and the gulf of mexico to the north . \ldots

};
\visible<2>{
\node [xshift=3.5cm, rectangle, draw,thick,fill=blue!0,text width=8em, rounded corners, inner sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize tabasco and chiapas states hardest hit. authorities say 700,000 affected \ldots   \par};}
\end{tikzpicture}
  \end{center}
\end{frame}


\begin{frame}
\thetitle{Motivation: \\
  Six Challenges for NMT (Koehn and Knowles 2017)}
\begin{itemize}
    \item \textbf{2: Requires large sample complexity}
    \air
    \item \textbf{5: The alignments learned by soft attention may not
        be interpreted as word alignments}
    \air
\end{itemize}
These issues are are even greater for text generation.
\end{frame}




% \begin{frame}
% \thetitle{This Work: A Study of Attention Models}
%      General method with many applications in,
%     \begin{enumerate}
%         \item \textbf{Translation}
%         \item Speech Recognition
%         \item Summarization
%         \item Image Generation
%     \end{enumerate}
%     \air

% \begin{itemize}
%     \item Can they be improved?
%     \item Can we incorporate external information?
%     \item Can we build in inductive bias?
% \end{itemize}
% \end{frame}

% \section{Formal Description}
\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn3}
\end{frame}


\begin{frame}\thetitle{Neural Attention versus Latent Alignment}
\begin{itemize}
     \item Attention is motivated as an approximation to alignment:
        \begin{itemize}
            \item makes the model's behavior interpretable
            \item can be used for other prediction tasks
        \end{itemize}
    \air
    % \item Soft Attention is \textit{deterministic}  from a neural network,
    \air

    \item We contrast this with latent alignment which acts as a random variable.
      \air

    \end{itemize}
    \centerline{\textbf{Key Distinction}}
    \begin{itemize}
    \item Both can compute $p(x | z)$ but attention cannot compute
      posterior $p(z | x)$.
    \end{itemize}
\end{frame}



\begin{frame}\thetitle{Latent Alignment: Motivation}

If attention works so well, why study alignment?

\begin{itemize}
    \item A latent variable approach  facilitates \structure{composibility}  in a principled probabilistic manner. (Cohn et al, 2016)
    \air
    \item \structure{Posterior inference} provides better post-hoc interpretability and analysis
    \air
    \item Modeling \structure{uncertainties} might lead to better performance
\end{itemize}
\end{frame}

\begin{frame}\thetitle{Notation: Alignment Model}
\begin{itemize}
    \item $x$ = $x_1,\cdots, x_T$: the observed set (the encoded source words
        and previous target words)
    \item $\tilde{x}$: the query (the decoder hidden state at a single timestep)
    \item $y$: the output (the current target word)
    \item $z$: the latent alignment, random variable indicating which member of $x$ generates $y$
\air
\begin{figure}
\centering
\begin{tikzpicture}
\node(z)[latent]{$z$};
\node(y)[right =of z, latent]{$y$};
\node(x)[obs, above = of y]{$x_i$};
\node(xp)[above= of z, obs]{$\tilde{x}$};
\plate {} {(x)} {};
\edge {x} {y};
\edge {xp} {z};
\edge {x} {z};
\edge {x} {y};
\edge {z} {y};
\end{tikzpicture}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}\thetitle{Problem Setup}
\begin{itemize}
\item Let $\mcD$ be the prior distribution of $z$ and $f(x,z;\theta)$ the
likelihood of $x$ given $z$
\item Generative Process
    \begin{eqnarray*}
        z \sim {\mcD}(a(x, \tilde{x};\theta )) \ \ \ \  y \sim f(x, z; \theta)
    \end{eqnarray*}
\item Training Objective (maximizing marginal log-likelihood)
    \begin{eqnarray*}
        \max_{\theta}\ \log p(y = \hat{y} \given  x, \ \tilde{x}) &=& \max_{\theta} \ \log \E_z [f(x, z;\theta)_{\hat{y}}]
    \end{eqnarray*}
\item Direct optimization is computationally expensive
    \begin{itemize}
        \item Discrete $z\sim{\mcD}$: $O(T)$ additional runtime
        \item Continuous ${z\sim\mcD}$: intractable
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}\thetitle{Workaround 1: Soft Attention}
\begin{itemize}
\item Replace the joint distribution with a nested expectation
{\small[Bahdanau et al 2014]}
    \begin{eqnarray*}
        \log \E_z [f(x, z)_{\hat{y}}] \approx \log f(x,\E_z[z])_{\tilde{y}}
    \end{eqnarray*}
\item The corresponding graphical model is
\air
\begin{figure}
        \centering
        \begin{tikzpicture}
            \node(y)[latent]{$y$};
            \node(x)[obs, above = of y]{$x_i$};
            \node(xp)[left= of x, obs]{$\tilde{x}$};
            \plate {} {(x)} {};
            \edge {xp} {y};
            \edge {x} {y};
        \end{tikzpicture}
    \end{figure}
\end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
    \textbf{Seq2Seq+} \air

  \end{center}
\center
\vspace{-5mm}
 \air
\includegraphics[scale=0.37]{nmt-attn4}
\end{frame}


\begin{frame}\thetitle{Workaround 2: Hard Attention}
\begin{itemize}
\item Keep the latent variable model formulation and maximize a lower bound
on the marginal likelihood
\air
\item {\small[Xu et al 2015]}: Directly apply Jensen's inequality and optimize
    with REINFORCE by sampling from the prior
    \begin{eqnarray*}
        \log \E_z [f(x, z)_{\hat{y}}] \ge \E_z \log [f(x, z)_{\hat{y}}] \approx \log f(x, \tilde{z})_{\hat{y}}
    \end{eqnarray*}
\item The use of the prior in the expectation may result in a poor bound
\item Cannot directly use for posterior estimation.

\end{itemize}
\end{frame}


\begin{frame}
\thetitle{Marginal Likelihood: Variational Decomposition}

For any\footnote{Technical condition: $\text{supp}(q(z)) \subset \text{supp}(p(z \given x ; \theta))$} distribution $q(z)$ over $z$,
\begin{align*}
 L(\theta) &= \textcolor{red}{\E_{q}\Big[\log p(y \given x, z) \Big] - \KL[q(z) \, \Vert \, p(z
  \given x, \tilde{x})] } \\ &
+ \textcolor{blue}{\KL[q(z )  \, \Vert \, p(z \given y, x, \tilde{x} )]}
\end{align*}

\begin{center}
\begin{tikzpicture}
    \draw[decorate,decoration={brace,amplitude=10pt}] (-0.3, 0) -- node[xshift=-0.75cm]{} (-0.3, 3);

    \draw[red] (0, 0) rectangle (1, 2);
    \draw[blue] (0, 2) rectangle (1, 3);
    \draw[decorate,decoration={brace,amplitude=5pt}] (1.3, 2) -- node[xshift=3cm]{$\alert{\text{ELBO  (evidence lower bound) }}$} (1.3, 0);
    \draw[decorate,decoration={brace,amplitude=5pt}] (1.3, 3) -- node[xshift=1.5cm]{$\structure{\text{posterior gap}}$} (1.3, 2);
\end{tikzpicture}
\end{center}

%\begin{itemize}
%    \item The \alert{evidence lower bound} (ELBO)
%    \item The \structure{posterior gap}
%\end{itemize}

Since KL is always non-negative, $L(\theta) \geq \text{ELBO}(\theta, \lambda)$.
\end{frame}


\begin{frame}\thetitle{Proposal: Variational Attention}
\begin{itemize}
    % \item Note that we must have $\textrm{supp } q(z) \subseteq \textrm{supp } p(z \given x, \tilde{x}, y)$
    \item Learn model and $q$ to maximize the following lower bound
        \begin{align*}
        \log &\E_{z \sim p(z \given x, \tilde{x})}[p(y \given x, z)]\\
        &\ge\E_{z \sim q(z)}[\log p(y \given x, z)] - \KL[q(z) \, \Vert \, p(z
  \given x, \tilde{x})]
        \end{align*}
    \item We choose a pair $\mcD$ and $q(z)$ that affords analytic KL
    \item At test time, marginalize over $z$ during decoding
\end{itemize}
\end{frame}


\begin{frame}
\thetitle{Example Form of $q$: Amortized Parameterization}
\begin{center}
    \begin{tikzpicture}
% nodes
 \node[latent] (zl) {$z$};%
 \node[obs, below=of zl] (xl) {$\tilde{x}$};%
 \node[obs, right=of xl] (xtilde) {$x_i$};%
 \node[obs, left=of xl] (yl) {$y$};%
 %\node[obs, right=1cm of dots] (xT) {$x_T^{(n)}$};%
 \node[const, above=of zl] (lambda) {$\lambda$};
% plate
 \edge{lambda}{zl};
 \edge{yl}{zl};
 \edge{xl}{zl};
\edge {xtilde} {z};
\plate {} {(xtilde)} {};

 \begin{scope}[xshift=5cm]
\node(z)[latent]{$z$};
\node(y)[right =of z, latent]{$y$};
\node(x)[obs, above = of y]{$x_i$};
\node(xp)[above= of z, obs]{$\tilde{x}$};
\plate {} {(x)} {};
\edge {x} {y};
\edge {xp} {z};
\edge {x} {z};

\edge {x} {y};
\edge {z} {y};
 \end{scope}
 \draw[dashed] (zl) --node [yshift=0.2cm] {$\KL(q(z) || p(z \given x,  \tilde{x}, y))$} (z);
\end{tikzpicture}
\end{center}
$\lambda$ parameterizes a global network (encoder) that is run over $x, y, \tilde{x}$
    to produce the local variational distribution, e.g.
    \[ q(z ; \lambda) = \text{enc}(x, \tilde{x}, y ; \lambda)\]
\end{frame}

\begin{frame}\thetitle{Method}
\begin{figure}
  \centering

  \begin{tikzpicture}[every node/.style={anchor=base,minimum size=8mm}]
    \matrix  (graph) [matrix of nodes, row sep=0.5em,column sep=-0.3em,
    minimum width=0.2em, minimum height=0.5em, font=\small,ampersand replacement=\&] {
      Mary \&
      did \&
      not \&
      slap \&
      the \&
      green \&
      witch \\
      $z$ \& \& \& \& \& \& $\substack{\textcolor{blue}{p(z| x, \tilde{x})} \\ \textcolor{red}{q(z; x, \tilde{x}, y)} }$ \\
      Maria \&
      no \&
      \textbf{daba} \&
      una \&
      bofetada \; a\&
       la \; bruja \&
      verda\\
    };


    \begin{scope}[on background layer]
      \draw[blue!30, line width=0.1mm] (graph-1-1) -- (graph-3-3);
      \draw[blue!30,line width=0.8mm] (graph-1-2) -- (graph-3-3);
      \draw[blue!30,line width=0.4mm] (graph-1-3) -- (graph-3-3);
      \draw[blue!30,line width=0.6mm] (graph-1-4) -- (graph-3-3);
      \draw[blue!30,line width=0.04mm] (graph-1-5) -- (graph-3-3);
      \draw[blue!30,line width=0.5mm] (graph-1-6) -- (graph-3-3);
      \draw[blue!30,line width=0.15mm] (graph-1-7) -- (graph-3-3);

      \draw[draw=red!30, line width=0.2mm] ($(graph-1-1) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      \draw[draw=red!30, line width=0.2mm] ($(graph-1-2) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      \draw[draw=red!30, line width=0.1mm] ($(graph-1-3) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      \draw[draw=red!30, line width=0.9mm] ($(graph-1-4) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      \draw[draw=red!30, line width=0.1mm] ($(graph-1-5) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      \draw[draw=red!30, line width=0.4mm] ($(graph-1-6) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      \draw[draw=red!30, line width=0.1mm] ($(graph-1-7) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);


      \draw[rounded corners,fill=red!10] ($ (graph-3-1.north west) +(-0.1,0.1)$) rectangle  node[yshift=-0.8cm]{\textcolor{red}{}} ($(graph-3-7.south east) +(0.1,-0.1)$ ) ;

      \draw[rounded corners,fill=red!10] ($ (graph-1-1.north west) +(-0.1,0.1)$) rectangle  node[yshift=0.8cm]{\textcolor{red}{}} ($(graph-1-7.south east) +(0.1,-0.1)$ ) ;

      \draw[rounded corners,fill=green!10] (graph-1-1.north west) rectangle  node[ yshift =0.7cm] {$x_{1:T}$} (graph-1-7.south east);

      \path[] (graph-3-3.north west) rectangle  node[yshift=-0.9cm]{\textbf{$y_3$}} (graph-3-3.south east);

      \draw[rounded corners, fill=blue!10] (graph-3-1.north west) rectangle  node[yshift=-0.9cm]{\textcolor{blue}{$\tilde{x}$}}  ($(graph-3-2.south east) + (-0.1, 0)$);

    \end{scope}
  \end{tikzpicture}


  A sketch of variational attention applied to translation.
  \begin{itemize}
  \item  The blue prior $p$ is restricted to past information,
  \item  The red variational posterior $q$ may take into account future observations.
  \end{itemize}
\end{figure}
\end{frame}


\begin{frame}\thetitle{Proposal: Categorical and Relaxed}
\begin{itemize}
    \item Categorical (Single Source Alignment Word)
    \begin{itemize}
        \item $z\sim\mcD$ and $q(z)$: Categorical
        \item Estimate gradients with REINFORCE
        \[ \E_{z \sim q(z)} [\nabla_\theta\log f(x,z) + \log f(x,z)\nabla_\phi \log q(z) ] \]
    \end{itemize}
    \pause

    \item Relaxed (Mixture Source Alignment)
    \begin{itemize}
        \item $z\sim\mcD$ and $q(z)$: Dirichlet
        \item Use the reparameterization trick {\small[Kingma et al 2013]}
            \begin{itemize}
            \item Sample $u$ from a simple distribution $\mathcal{U}$
            \item Apply transformation $g_{\phi}(\cdot)$ to obtain $z=g_{\phi}(u)$
            \end{itemize}
        \item The gradient estimator takes the form
            \[ \E_{u \sim \mathcal{U}} \left[\nabla_{\theta,\phi} \log f(x,g_\phi(u))\right ]\]
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}\thetitle{Important Extension: Variance Reduction for Categorical}
\begin{itemize}
    \item REINFORCE gradient estimator suffers from high variance
    \air
    \item Introduce control variate or baseline $B = \log f(x, \E_{z'\sim q(z)}[z'])$ from soft attention
    \[ \E_{z \sim q(z)} [\nabla_\theta\log f(x,z) + ( \log f(x,z) -
  B)\nabla_\phi \log q(z) ] \]
    \item Requires a single additional evaluation of $f(x,\E_{z'\sim q(z)}[z'])$
\end{itemize}
\end{frame}


\begin{frame}
  \thetitle{Concurrent Experimental Work}

  Many different researchers have recently explored the
  benefits of marginalization. Very similar results.

  \begin{itemize}
  \item Surprisingly Easy Hard-Attention for Sequence to Sequence Learning
  \item Hard Non-Monotonic Attention for Character-Level Transduction
  \item Posterior Attention Models for Sequence to Sequence Learning
  \end{itemize}

  These results seem robust across these works.

\end{frame}

\begin{frame}
  \thetitle{Experiments}

  \begin{itemize}
  \item Full experiments on IWSLT and WMT using LSTM based NMT system.
  \item Model: Two layer attention based LSTM.
  \item Variational Model: Bidirectional LSTM model.
  \item Preliminary experiments on low-resource MT setup.
  \end{itemize}

\end{frame}


\begin{frame}\thetitle{Results (MT: IWSLT)}
\begin{table}
  \centering
  \begin{tabular}{lllrrrrr}
    \toprule
    % %
    Model & Objective & Exp  & PPL &  BLEU \\
    \midrule
  Soft Attn & $\log p(y \given \E[z])$ & Softmax & 7.17 &  32.77 \\
  Marg. Likelihood & $\log \E[p]$ &  Enum & 6.34  & 33.29\\
  Hard Attn  & $\E_p[\log p]$ & Enum & 6.77 &  31.40\\
  Hard Attn  & $\E_p[\log p]$  & Sample &  6.78 &   30.42\\
  Var Relaxed Attn& $\E_q[\log p] - \KL$ & Sample & 7.58	 & 30.05 \\
  Var Attn  & $\E_q[\log p] - \KL$ & Enum & 6.08&  \textbf{33.69} \\
  Var Attn &  $\E_q[\log p] - \KL$ &Sample& 6.17&  \textbf{33.30} \\
    \bottomrule
  \end{tabular}

  % \caption{The Exp column indicates
  % whether the expectation in the objective is calculated via enumeration (Enum) or approximated with a single sample (Sample) during training.
  % We evaluate on perplexity PPL (lower is better)
  % and BLEU (higher is better) for IWSLT '14 De-En.}
\end{table}
\end{frame}

\begin{frame}\thetitle{Results (MT: WMT)}
\begin{table}
  \centering
  \begin{tabular}{lllrrrrr}
    \toprule
    % %
    Model & Objective & Exp  & PPL &  BLEU \\
    \midrule
  Soft Attn & $\log p(y \given \E[z])$ & Softmax & - &  24.10 \\
  Var Attn &  $\E_q[\log p] - \KL$ &Sample& - &  24.98 \\
    \bottomrule
  \end{tabular}

  % \caption{The Exp column indicates
  % whether the expectation in the objective is calculated via enumeration (Enum) or approximated with a single sample (Sample) during training.
  % We evaluate on perplexity PPL (lower is better)
  % and BLEU (higher is better) for IWSLT '14 De-En.}
\end{table}
\end{frame}

\begin{frame}\thetitle{Preliminary Results (Low Data Settings)}
\begin{table}
  \centering
  \begin{tabular}{lllrrrrr}
    \toprule
    % %
    Training Size & 10k  & 25k  & 50k &  Full (160k)\\
    \midrule
  Soft Attn           & 12.10 & 22.88 & 26.65 & 32.77 \\
  Marginal Likelihood & 16.79 & 23.44 & 26.99 & 33.29 \\
  Var Attn Enum       & 14.90 & 23.50 & 27.26 &  33.69 \\
  Var Attn Sample     & 12.20 & 23.35 & 27.87 &  33.30 \\
    \bottomrule
  \end{tabular}
\end{table}
\end{frame}
\begin{frame}\thetitle{Results (VQA)}
\begin{table}
  \centering
  \begin{tabular}{lllrrrrr}
    \toprule
    % %
    Model & Objective & Exp  & NLL &  Eval \\
    \midrule
  Soft Attn & $\log p(y \given \E[z])$ & Softmax & 1.76 & 58.93 \\
  Marg. Likelihood & $\log \E[p]$ &  Enum & 1.69  &60.33\\
  Hard Attn  & $\E_p[\log p]$ & Enum & 1.78 &  57.60\\
  Hard Attn  & $\E_p[\log p]$  & Sample &  1.82 &   56.30\\
  Var Attn  & $\E_q[\log p] - \KL$ & Enum & 1.68 &  58.44 \\
  Var Attn &  $\E_q[\log p] - \KL$ &Sample& 1.74 &  57.52 \\
    \bottomrule
  \end{tabular}

  \caption{The Exp column indicates
  whether enumeration (Enum) or sampling (Sample) was used during training.
  We evaluate intrinsically on negative log-likelihood NLL (lower is better)
  and VQA evaluation metric (higher is better).}
   \label{tab:eval_nmt}
\end{table}
\end{frame}


\begin{frame}\thetitle{Inference}
\begin{center}
\includegraphics[width=10cm]{sample}
\end{center}

\end{frame}

\begin{frame}\thetitle{Discussion}
\begin{itemize}
    \item Soft attention underperforms
        latent attention
    \air
    \item Variational attention achieves
        similar performance to maximizing the marginal likelihood exactly
        despite optimizing a lower bound
    \air
    \item Proposed soft attention as a computationally cheap baseline for reducing the variance of the
        REINFORCE gradient estimator
\end{itemize}
\end{frame}

\begin{frame}\thetitle{Discussion: Alternative Inference Methods}
\begin{table}
\centering
\begin{tabular}{lccc}
       \toprule
    Inference Method & \#Samples & PPL & BLEU \\
    \midrule
  REINFORCE & 1 & 6.17 & 33.30 \\
  RWS & 5 & 6.41 & 32.96 \\
  Gumbel-Softmax  & 1 & 6.51 & 33.08\\
    \bottomrule
   \end{tabular}
   \end{table}
\begin{itemize}
    \item Gumbel-Softmax is a viable alternative
    \item RWS incurs higher memory cost
\end{itemize}


\end{frame}

\begin{frame}\thetitle{Example Alignments}
\begin{figure}[t]
  \centering
  \includegraphics[height=7cm]{img/var-attn/2531p-q-attn}
  \caption{\label{fig:attn} Red: prior; blue: posterior.}
  \label{fig:pq}
\end{figure}
\end{frame}
\begin{frame}\thetitle{Example Alignments}
\begin{figure}[t]
  \centering
  \includegraphics[height=7cm]{img/var-attn/2509svattn}
  \label{fig:pq:b}
  \caption{\label{fig:attn} Red: prior; green: soft attention.}
  \label{fig:pq}
\end{figure}
\end{frame}

\begin{frame}
  \thetitle{Conclusion}
  \begin{itemize}
  \item Discrete latent variables are a natural fit for NLP
    \air
  \item Methods can be integrated naturally into many problems.
    \air
  \item Tools for training these approaches are coming along fast.
    \air

  \item Also check out our tutorial: Deep Latent-Variable NLP
    \url{bit.do/lvnlp}

  \end{itemize}
\end{frame}

\end{document}
